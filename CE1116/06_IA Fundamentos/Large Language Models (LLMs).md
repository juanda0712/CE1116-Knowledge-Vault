# **Large Language Models (LLMs)**

Un Large Language Model (LLM) es un modelo de Deep Learning entrenado con enormes vol칰menes de texto y miles de millones de par치metros, dise침ado para predecir la siguiente palabra en una secuencia, lo que le permite capturar patrones complejos y conocimiento del mundo.
Se basan en la arquitectura Transformer, cuyo n칰cleo es el mecanismo de autoatenci칩n, que identifica qu칠 partes del texto son relevantes en cada paso. Cada bloque incluye Feed-Forward Networks (FFN) para abstraer patrones de alto nivel, as칤 como conexiones residuales y Layer Normalization que estabilizan el entrenamiento. Las palabras se representan mediante embeddings y se aplica codificaci칩n posicional para preservar el orden de la secuencia. La generaci칩n de texto utiliza Softmax y puede ajustarse con la temperatura para controlar la diversidad de las predicciones. En decodificadores como GPT, se emplea atenci칩n causal para evitar que tokens futuros influyan en los presentes.
El entrenamiento se realiza en dos fases: pre-entrenamiento (aprendizaje general) y fine-tuning (adaptaci칩n espec칤fica). Durante el entrenamiento, se ajustan los par치metros; durante la inferencia, se genera texto a partir del modelo ya entrenado. Los Transformers permiten paralelizaci칩n masiva, aunque la ventana de contexto finita limita la memoria en interacciones largas. A escalas masivas, los LLMs desarrollan habilidades emergentes, como razonamiento complejo, que van m치s all치 de su objetivo de predicci칩n de palabras.

## 游닄 Idea/Concepto

Modelos de aprendizaje profundo con miles de millones de par치metros, entrenados en grandes cantidades de texto, que aprenden patrones estad칤sticos y contextuales del lenguaje para generar, resumir o razonar sobre texto.

## 游늷 Puntos Claves (Opcional)

- Escalan en par치metros (ej. GPT-3 con 175B).
- Usan arquitecturas Transformer.
- Se adaptan a tareas con pocas instrucciones (few-shot).

## 游댕 Connections

- [[Redes Neuronales]]
- [[Mecanismo de atenci칩n utilizado en la arquitectura de Transformers]]
- [[Ventana de Contexto del modelo]]
