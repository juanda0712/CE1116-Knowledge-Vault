# **Embeddings**

Los embeddings son representaciones num√©ricas densas y aprendibles de tokens o unidades de informaci√≥n en un espacio vectorial de varias dimensiones, dise√±adas para capturar similitudes sem√°nticas y relaciones contextuales. Cada embedding es un vector de par√°metros aprendibles, ajustado durante el entrenamiento mediante minimizaci√≥n de una funci√≥n de coste y retropropagaci√≥n.
En los Transformers, los embeddings iniciales funcionan como una tabla de consulta (lookup table) y se refinan progresivamente mediante los bloques de atenci√≥n, generando embeddings contextuales, donde la representaci√≥n de un token depende de su contexto en la frase (por ejemplo, ‚Äúbanco‚Äù de r√≠o vs. ‚Äúbanco‚Äù financiero). La dimensionalidad de los embeddings (p. ej., 512, 768, 12288) es un hiperpar√°metro cr√≠tico que determina la riqueza de la representaci√≥n.
Los embeddings incluyen informaci√≥n posicional (Positional Encoding) para que el modelo comprenda el orden de los tokens, dado que los Transformadores carecen de recurrencia. Los vectores de Query, Key y Value se derivan mediante proyecciones lineales de los embeddings (multiplicados por matrices WQ, WK, WV) antes de calcular la atenci√≥n.
Adem√°s, los embeddings pueden ser multimodales, integrando texto, im√°genes y audio en un mismo espacio vectorial, permitiendo un procesamiento integral y contextualizado. Por ejemplo, un modelo puede analizar una imagen de comida y generar consejos nutricionales relevantes combinando informaci√≥n visual y textual.
Esta definici√≥n se centra en embeddings densos y aprendibles, que son la base de los modelos modernos de lenguaje, sin profundizar en representaciones dispersas anteriores (como one-hot encoding), manteniendo claridad y relevancia t√©cnica.

## üìö Idea/Concepto

Representaciones vectoriales densas de tokens, palabras o unidades de entrada en un espacio continuo de alta dimensi√≥n, donde la cercan√≠a entre vectores refleja similitud sem√°ntica o sint√°ctica.

## üìå Puntos Claves (Opcional)

- Generados inicialmente por lookup tables y refinados contextualmente.
- Dimensionalidad configurable (ej. 512, 768, 12288).
- Incluyen codificaci√≥n posicional para mantener el orden.

## üîó Connections

- [[Tokenizacion]]
- [[Mecanismo de atenci√≥n utilizado en la arquitectura de Transformers]]
- [[Redes Neuronales]]
