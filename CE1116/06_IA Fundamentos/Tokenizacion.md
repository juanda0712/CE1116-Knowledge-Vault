# **Tokenizaci贸n**

La tokenizaci贸n es el proceso fundamental de preprocesamiento en modelos de lenguaje y arquitecturas multimodales. Consiste en segmentar la entrada (texto, audio, imagen o video) en unidades m铆nimas llamadas tokens, que representan la base operativa del modelo. En el caso del texto, cada token se convierte en un ID num茅rico 煤nico dentro de un vocabulario y, posteriormente, se proyecta en un vector denso mediante la matriz de embeddings (W_E), la cual funciona como una tabla de consulta y cuyos valores se aprenden y ajustan durante el entrenamiento para capturar relaciones sem谩nticas.
Los m茅todos modernos de tokenizaci贸n, como BPE y WordPiece, permiten dividir palabras en subunidades, lo que reduce el tama帽o del vocabulario, mejora la representaci贸n de palabras raras y ofrece una soluci贸n eficiente al problema de las palabras fuera de vocabulario (OOV). Esta estrategia asegura un equilibrio entre la compacidad del vocabulario y la fidelidad sem谩ntica.
En arquitecturas actuales, el concepto de token trasciende el texto y se extiende a segmentos de otras modalidades (p铆xeles de im谩genes, espectrogramas de audio, fragmentos de video), lo que permite integrar diferentes tipos de datos bajo un marco unificado. De esta manera, la tokenizaci贸n no solo facilita la entrada al modelo, sino que tambi茅n define la granularidad de la informaci贸n y constituye el puente entre datos crudos y representaciones vectoriales que impulsan el aprendizaje profundo.

##  Idea/Concepto

Proceso de dividir un texto en unidades m谩s peque帽as llamadas tokens, que pueden ser caracteres, subpalabras o palabras completas, y que sirven como entrada a los modelos de lenguaje.

##  Puntos Claves (Opcional)

- M茅todos comunes: BPE, WordPiece, SentencePiece.
- Define cu谩ntos tokens ocupa una oraci贸n.
- Afecta directamente la eficiencia y precisi贸n del modelo.

##  Connections

- [[Embeddings]]
- [[Ventana de Contexto del modelo]]
- [[Large Language Models (LLMs)]]
