# **Mecanismo de atenci贸n utilizado en la arquitectura de Transformers**

El mecanismo de atenci贸n permite que un modelo enfoque su procesamiento en las partes m谩s relevantes de la informaci贸n. Cada token del input se proyecta en tres vectores: query (Q), key (K) y value (V), generados mediante matrices de pesos aprendibles (WQ, WK, WV) aplicadas a los embeddings de entrada. Los pesos de atenci贸n se calculan mediante el producto punto de Q y K, escalado por dk para estabilidad num茅rica, y normalizados con Softmax, luego aplicados a V para obtener la representaci贸n final de cada token.
En Transformers, se usa self-attention, donde cada token atiende a todos los dem谩s para capturar dependencias largas. El mecanismo se implementa como multi-head attention, donde cada cabeza aprende diferentes relaciones (sint谩cticas, sem谩nticas, contextuales); las salidas se concatenan y se proyectan linealmente mediante WO. Para decodificadores autorregresivos, se aplica enmascaramiento causal para garantizar generaci贸n secuencial correcta.
El mecanismo forma parte de los bloques Transformer, que incluyen Feed-Forward Networks (FFN) para abstracci贸n de alto nivel, as铆 como conexiones residuales y Layer Normalization para estabilidad del entrenamiento. Aunque es altamente paralelizable, su complejidad es O(n虏路d) respecto a la longitud de la secuencia, limitando la ventana de contexto de los LLMs.

##  Idea/Concepto

Proceso mediante el cual un modelo asigna diferentes pesos a cada parte de una secuencia al procesar un token, permiti茅ndole enfocarse en las partes m谩s relevantes para la tarea.

##  Puntos Claves (Opcional)

- Se basa en el c谩lculo de similitud entre vectores Query, Key y Value.
- Atenci贸n escalada por el producto punto (Scaled Dot-Product Attention).
- Permite paralelismo y sustituye recurrencia.

##  Connections

- [[Large Language Models (LLMs)]]
- [[Embeddings]]
- [[Ventana de Contexto del modelo]]
